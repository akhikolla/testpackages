<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Matt Moores, Anthony N. Pettitt and Kerrie Mengersen" />

<meta name="date" content="2019-01-04" />

<title>bayesImageS: An R Package for Bayesian Image Segmentation using a Hidden Potts Model</title>






<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>

</head>

<body>




<h1 class="title toc-ignore">bayesImageS: An R Package for Bayesian Image Segmentation using a Hidden Potts Model</h1>
<h4 class="author"><em>Matt Moores, Anthony N. Pettitt and Kerrie Mengersen</em></h4>
<h4 class="date"><em>2019-01-04</em></h4>



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Image segmentation can be viewed as the task of labelling the observed pixels <span class="math inline">\(\mathbf{y}\)</span> according to a finite set of discrete states <span class="math inline">\(\mathbf{z} \in \{ 1, \dots, k \}\)</span>. The hidden  model allows for spatial correlation between neighbouring labels in the form of a Markov random field. The latent labels follow a Gibbs distribution, which is specified in terms of its conditional probabilities: <span class="math display">\[\begin{equation}
  \label{eq:Potts}
  p(z_i | z_{\setminus i}, \beta) = \frac{\exp\left\{\beta\sum_{i \sim \ell}\delta(z_i,z_\ell)\right\}}{\sum_{j=1}^k \exp\left\{\beta\sum_{i \sim \ell}\delta(j,z_\ell)\right\}}
  \end{equation}\]</span> where <span class="math inline">\(\beta\)</span> is the inverse temperature, <span class="math inline">\(z_{\setminus i}\)</span> represents all of the labels except <span class="math inline">\(z_i\)</span>, <span class="math inline">\(i \sim \ell\)</span> are the neighbouring pixels of <span class="math inline">\(i\)</span>, and <span class="math inline">\(\delta(u,v)\)</span> is the Kronecker delta function. Thus, <span class="math inline">\(\sum_{i \sim \ell}\delta(z_i,z_\ell)\)</span> is a count of the neighbours that share the same label.</p>
<p>The observation equation links the latent labels to the corresponding pixel values: <span class="math display">\[\begin{equation}
  \label{eq:obs}
  p(\mathbf{y} | \mathbf{z}, \boldsymbol\theta) = \prod_{i=1}^n p(y_i | z_i, \theta_{z_i})
  \end{equation}\]</span> where <span class="math inline">\(\theta_{j}\)</span> are the parameters that govern the distribution of the pixel values with label <span class="math inline">\(j\)</span>. The hidden Potts model can thus be viewed as a spatially-correlated generalisation of the finite mixture model . We assume that the pixels with label <span class="math inline">\(j\)</span> share a common mean <span class="math inline">\(\mu_j\)</span> corrupted by additive Gaussian noise with variance <span class="math inline">\(\sigma_j^2\)</span>: <span class="math display">\[\begin{equation}
  \label{eq:obs2}
y_i | z_i=j, \mu_j, \sigma^2_j \;\sim\; \mathcal{N}\left( \mu_j, \sigma^2_j \right)
  \end{equation}\]</span></p>
<p>The Gibbs distribution is a member of the exponential family and so there is a sufficient statistic for this model, as noted by : <span class="math display">\[\begin{equation}
  \label{eq:potts_stat}
\mathrm{S}(\mathbf{z}) = \sum_{i \sim \ell \in \mathcal{E}} \delta(z_i,z_\ell)
  \end{equation}\]</span> This statistic represents the total number of like neighbour pairs in the image. The likelihood <span class="math inline">\(p(\mathbf{y},\mathbf{z} | \boldsymbol\theta, \beta)\)</span> can therefore be factorised into <span class="math inline">\(p(\mathbf{y} | \mathbf{z}, \boldsymbol\theta) p(\mathrm{S}(\mathbf{z}) | \beta)\)</span>, where the second factor does not depend on the observed data, but only on the sufficient statistic. The joint posterior is then: <span class="math display">\[\begin{equation}
\label{eq:joint_post}
p(\boldsymbol\theta, \beta, \mathbf{z} | \mathbf{y}) \propto p(\mathbf{y} | \mathbf{z}, \boldsymbol\theta) \pi(\boldsymbol\theta) p(\mathrm{S}(\mathbf{z}) | \beta) \pi(\beta)
\end{equation}\]</span> The conditional distributions <span class="math inline">\(p(\boldsymbol\theta | \mathbf{z}, \mathbf{y})\)</span> and <span class="math inline">\(p(z_i | z_{\setminus i}, \beta, y_i, \boldsymbol\theta_{z_i})\)</span> can be simulated using Gibbs sampling, but <span class="math inline">\(p(\beta | \mathbf{y}, \mathbf{z}, \boldsymbol\theta)\)</span> involves an intractable normalising constant <span class="math inline">\(\mathcal{C}(\beta)\)</span>: <span class="math display">\[\begin{eqnarray}
  \label{eq:beta_post}
  p(\beta \mid \mathbf{y}, \mathbf{z}, \boldsymbol\theta) &amp;=&amp;  \frac{p(\mathrm{S}(\mathbf{z}) | \beta) \pi(\beta)}{\int_\beta p(\mathrm{S}(\mathbf{z}) | \beta) \pi(d \beta)}
\\   \label{eq:beta}
 &amp;\propto&amp; \frac{\exp\left\{ \beta\, \mathrm{S}(\mathbf{z}) \right\}}{\mathcal{C}(\beta)} \pi(\beta)
  \end{eqnarray}\]</span> The normalising constant is also known as a partition function in statistical physics. It has computational complexity of <span class="math inline">\(\mathcal{O}(n k^n)\)</span>, since it involves a sum over all possible combinations of the labels <span class="math inline">\(\mathbf{z} \in \mathcal{Z}\)</span>: <span class="math display">\[\begin{equation}
  \label{eq:norm}
\mathcal{C}(\beta) = \sum_{\mathbf{z} \in \mathcal{Z}} \exp\left\{\beta\, \mathrm{S}(\mathbf{z})\right\}
  \end{equation}\]</span> It is infeasible to calculate this value exactly for nontrivial images, thus computational approximations are required.</p>
<p>This paper describes the  package , which is available from the Comprehensive R Archive Network (CRAN) at <a href="https://CRAN.R-project.org/package=bayesImageS" class="uri">https://CRAN.R-project.org/package=bayesImageS</a>. This package implements five major algorithms for intractable likelihoods in Bayesian image analysis. These methods provide alternative means to simulate parameter values from () without computing the normalising constant. We describe the algorithms in terms of Markov chain Monte Carlo (MCMC) to enable direct comparison, although we also mention other approaches where applicable, such as particle-based (SMC and PMCMC) methods. Reference implementations of all of these methods are available from various sources described below, but for the purpose of comparison we have reimplemented the algorithms using  .</p>
<p>There are a number of contributed packages available for , for example on CRAN, which provide image segmentation using Potts and other models: , , , </p>
</div>
<div id="informative-priors" class="section level1">
<h1>Informative Priors</h1>
<p>It can be useful to place informative priors on the parameters of the mixture components</p>
<p>The external field prior </p>
</div>
<div id="algorithms-for-intractable-likelihoods" class="section level1">
<h1>Algorithms for Intractable Likelihoods</h1>
<div id="pseudolikelihood-and-composite-likelihood" class="section level2">
<h2>Pseudolikelihood and Composite Likelihood</h2>
<p>Pseudolikelihood is the simplest of the methods that we have implemented and also the fastest.  showed that the intractable distribution () could be approximated using the product of the conditional densities given by (): <span class="math display">\[\begin{equation} \label{eq:pseudo}
p(\mathrm{S}(\mathbf{z}) | \beta) \approx \prod_{i=1}^n p(z_i | z_{\setminus i}, \beta)
\end{equation}\]</span> This enables updates for the inverse temperature at iteration <span class="math inline">\(t\)</span> to be simulated using a Metropolis-Hastings (M-H) step, with acceptance ratio: <span class="math display">\[\begin{equation} \label{mh:ratio}
\rho = \min\left( 1, \frac{p(\mathbf{z}|\beta') \pi(\beta') q(\beta_{t-1} | \beta')}{p(\mathbf{z}|\beta_{t-1}) \pi(\beta_{t-1}) q(\beta' | \beta_{t-1})} \right)
\end{equation}\]</span></p>
<p>The M-H proposal density <span class="math inline">\(q(\beta'|\beta_{t-1})\)</span> can be any distribution such that <span class="math inline">\(\int q(\beta'|\beta_{t-1})\, d\beta' = 1\)</span>. However, there is a tradeoff between exploring the full state space and ensuring that the probability of acceptance is sufficiently high. We use the adaptive random walk (RWMH) algorithm of , which automatically tunes the bandwidth of the proposal density to target a given M-H acceptance rate. When a symmetric proposal density is used, <span class="math inline">\(q(\beta'|\beta_{t-1}) = q(\beta_{t-1}|\beta')\)</span> and so this term cancels out in the M-H ratio . Likewise, under a uniform prior for the inverse temperature, <span class="math inline">\(\pi(\beta') = \pi(\beta_{t-1}) = 1\)</span>. The natural logarithm of <span class="math inline">\(\rho\)</span> is used in practice to improve numerical stability.</p>

<p>Pseudolikelihood is exact when <span class="math inline">\(\beta=0\)</span> and provides a reasonable approximation for small values of the inverse temperature. However, the approximation error increases rapidly for <span class="math inline">\(\beta \ge \beta_{crit}\)</span>, as illustrated by Figure . This is due to long-range dependence between the labels, which is inadequately modelled by the local approximation. The implications of this inaccuracy for posterior inference will be demonstrated in Section .</p>
<p> referred to Equation  as point pseudolikelihood, since the conditional distributions are computed for each pixel individually. They suggested that the accuracy could be improved using block pseudolikelihood. This is where the likelihood is calculated exactly for small blocks of pixels, then  is modified to be the product of the blocks: <span class="math display">\[\begin{equation}
p(\mathbf{z}|\beta) \approx \prod_{i=1}^{N_B} p(\mathbf{z}_{B_i} | \mathbf{z}_{\setminus B_i}, \beta)
\label{eq:pl_comp}
\end{equation}\]</span> where <span class="math inline">\(N_B\)</span> is the number of blocks, <span class="math inline">\(\mathbf{z}_{B_i}\)</span> are the labels of the pixels in block <span class="math inline">\(B_i\)</span>, and <span class="math inline">\(\mathbf{z}_{\setminus B_i}\)</span> are all of the labels except for <span class="math inline">\(\mathbf{z}_{B_i}\)</span>. This is a form of composite likelihood, where the likelihood function is approximated as a product of simplified factors .  compared point pseudolikelihood to composite likelihood with blocks of <span class="math inline">\(3 \times 3\)</span>, <span class="math inline">\(4 \times 4\)</span>, <span class="math inline">\(5 \times 5\)</span>, and <span class="math inline">\(6 \times 6\)</span> pixels.  showed that () outperformed () for the Ising (<span class="math inline">\(k=2\)</span>) model with <span class="math inline">\(\beta &lt; \beta_{crit}\)</span>.  discuss composite likelihood for the Potts model with <span class="math inline">\(k &gt; 2\)</span> and have provided an open source implementation in the  package .</p>
<p>Evaluating the conditional likelihood in () involves the normalising constant for <span class="math inline">\(\mathbf{z}_{B_i}\)</span>, which is a sum over all of the possible configurations <span class="math inline">\(\mathcal{Z}_{B_i}\)</span>. This is a limiting factor on the size of blocks that can be used. The brute-force method that was used to compute Figure  is too computationally intensive for this purpose.  showed that the normalising constant can be calculated exactly for a cylindrical lattice by computing eigenvalues of a <span class="math inline">\(k^r \times k^r\)</span> matrix, where <span class="math inline">\(r\)</span> is the smaller of the number of rows or columns. The value of () for a free boundary lattice can then be approximated using path sampling.  extended this method to larger lattices using a composite likelihood approach.</p>
<p>The reduced dependence approximation (RDA) is another form of composite likelihood.  introduced a recursive algorithm to calculate the normalising constant using a lag-<span class="math inline">\(r\)</span> representation.  divided the image lattice into sub-lattices of size <span class="math inline">\(r_1 &lt; r\)</span>, then approximated the normalising constant of the full lattice using RDA: <span class="math display">\[\begin{equation}
\mathcal{C}(\beta) \approx \frac{\mathcal{C}_{r_1 \times n}(\beta)^{r - r_1 + 1}}{\mathcal{C}_{r_1 - 1 \times n}(\beta)^{r - r_1}}
\label{eq:rda}
\end{equation}\]</span>  compared RDA to pseudolikelihood and the exact method of , reporting similar computational cost to pseudolikelihood but with improved accuracy in estimating <span class="math inline">\(\beta\)</span>.  </p>
<p>Source code for RDA is available in the online supplementary material for .</p>
</div>
</div>
<div id="examples" class="section level1">
<h1>Examples</h1>
<p></p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>

<p>MTM was supported by the UK EPSRC as part of the <em>i</em>-Like programme grant (ref: EP/K014463/1). KLM was supported by an ARC Laureate Fellowship. KLM and ANP received funding from the ARC Centre of Excellence in Mathematical and Statistical Frontiers (ACEMS). Landsat imagery courtesy of NASA Goddard Space Flight Center and U.S. Geological Survey. Computational resources and services used in this work were provided by the HPC and Research Support Group, Queensland University of Technology, Brisbane, Australia.</p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
