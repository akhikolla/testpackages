
@article{altoeEnhancingStatisticalInference2020,
  title = {Enhancing {{Statistical Inference}} in {{Psychological Research}} via {{Prospective}} and {{Retrospective Design Analysis}}},
  author = {Altoè, Gianmarco and Bertoldo, Giulia and Zandonella Callegher, Claudio and Toffalini, Enrico and Calcagnì, Antonio and Finos, Livio and Pastore, Massimiliano},
  date = {2020},
  journaltitle = {Frontiers in Psychology},
  volume = {10},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.02893},
  url = {https://www.frontiersin.org/article/10.3389/fpsyg.2019.02893/full},
  urldate = {2020-01-29},
  abstract = {In the past two decades, psychological science has experienced an unprecedented replicability crisis, which has uncovered several issues. Among others, the use and misuse of statistical inference plays a key role in this crisis. Indeed, statistical inference is too often viewed as an isolated procedure limited to the analysis of data that have already been collected. Instead, statistical reasoning is necessary both at the planning stage and when interpreting the results of a research project. Based on these considerations, we build on and further develop an idea proposed by Gelman and Carlin (2014) termed “prospective and retrospective design analysis.” Rather than focusing only on the statistical significance of a result and on the classical control of type I and type II errors, a comprehensive design analysis involves reasoning about what can be considered a plausible effect size. Furthermore, it introduces two relevant inferential risks: the exaggeration ratio or Type M error (i.e., the predictable average overestimation of an effect that emerges as statistically significant) and the sign error or Type S error (i.e., the risk that a statistically significant effect is estimated in the wrong direction). Another important aspect of design analysis is that it can be usefully carried out both in the planning phase of a study and for the evaluation of studies that have already been conducted, thus increasing researchers’ awareness during all phases of a research project. To illustrate the benefits of a design analysis to the widest possible audience, we use a familiar example in psychology where the researcher is interested in analyzing the differences between two independent groups considering Cohen’s d as an effect size measure. We examine the case in which the plausible effect size is formalized as a single value, and we propose a method in which uncertainty concerning the magnitude of the effect is formalized via probability distributions. Through several examples and an application to a real case study, we show that, even though a design analysis requires significant effort, it has the potential to contribute to planning more robust and replicable studies. Finally, future developments in the Bayesian framework are discussed.},
  file = {/Users/claudio/MEGA/Zotero/Altoè et al_2020_Enhancing Statistical Inference in Psychological Research via Prospective and.pdf},
  langid = {english}
}

@report{bertoldoDesigningStudiesEvaluating2020,
  title = {Designing {{Studies}} and {{Evaluating Research Results}}: {{Type M}} and {{Type S Errors}} for {{Pearson Correlation Coefficient}}},
  shorttitle = {Designing {{Studies}} and {{Evaluating Research Results}}},
  author = {Bertoldo, Giulia and Zandonella Callegher, Claudio and Altoè, Gianmarco},
  date = {2020-06-17},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/q9f86},
  url = {https://osf.io/q9f86},
  urldate = {2020-07-31},
  abstract = {It is widely appreciated that many studies in psychological science suffer from low statistical power. One of the consequences of analyzing underpowered studies with thresholds of statistical significance, is a high risk of finding exaggerated effect size estimates, in the right or in the wrong direction. These inferential risks can be directly quantified in terms of Type M (magnitude) error and Type S (sign) error, which directly communicate the consequences of design choices on effect size estimation. Given a study design, Type M error is the factor by which a statistically significant effect is on average exaggerated. Type S error is the probability to find a statistically significant result in the opposite direction to the plausible one. Ideally, these errors should be considered during a prospective design analysis in the design phase of a study to determine the appropriate sample size. However, they can also be considered when evaluating studies’ results in a retrospective design analysis. In the present contribution we aim to facilitate the considerations of these errors in the research practice in psychology. For this reason we illustrate how to consider Type M and Type S errors in a design analysis using one of the most common effect size measures in psychology: Pearson correlation coefficient. We provide various examples and make the R functions freely available to enable researchers to perform design analysis for their research projects.},
  file = {/Users/claudio/Zotero/storage/N8NSKAGZ/Bertoldo et al. - 2020 - Designing Studies and Evaluating Research Results.pdf},
  langid = {english},
  type = {preprint}
}

@article{eisenbergerDoesRejectionHurt2003,
  title = {Does Rejection Hurt? {{An fMRI}} Study of Social Exclusion},
  author = {Eisenberger, Naomi I. and Lieberman, Matthew D. and Williams, Kipling D.},
  date = {2003},
  journaltitle = {Science},
  volume = {302},
  pages = {290--292},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.1089134},
  url = {https://science.sciencemag.org/content/302/5643/290},
  abstract = {A neuroimaging study examined the neural correlates of social exclusion and tested the hypothesis that the brain bases of social pain are similar to those of physical pain. Participants were scanned while playing a virtual ball-tossing game in which they were ultimately excluded. Paralleling results from physical pain studies, the anterior cingulate cortex (ACC) was more active during exclusion than during inclusion and correlated positively with self-reported distress. Right ventral prefrontal cortex (RVPFC) was active during exclusion and correlated negatively with self-reported distress. ACC changes mediated the RVPFC-distress correlation, suggesting that RVPFC regulates the distress of social exclusion by disrupting ACC activity.},
  number = {5643}
}

@book{gelman_hill_vehtari_2020,
  title = {Regression and Other Stories},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  date = {2020},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  collection = {Analytical Methods for Social Research},
  file = {/Users/claudio/Zotero/storage/78GJ64NB/Gelman et al. - Regression and Other Stories.pdf},
  keywords = {To read},
  series = {Analytical Methods for Social Research}
}

@article{gelmanPowerCalculationsAssessing2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  date = {2014},
  journaltitle = {Perspectives on Psychological Science},
  volume = {9},
  pages = {641--651},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691614551642},
  url = {http://journals.sagepub.com/doi/10.1177/1745691614551642},
  urldate = {2018-10-25},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  file = {/Users/claudio/MEGA/Zotero/Gelman_Carlin_2014_Beyond Power Calculations.pdf},
  keywords = {Effect Size,Letto,Power Analysis,Type S and Type M errror},
  langid = {english},
  number = {6}
}

@article{goodmanUsePredictedConfidence1994,
  title = {The {{Use}} of {{Predicted Confidence Intervals When Planning Experiments}} and the {{Misuse}} of {{Power When Interpreting Results}}},
  author = {Goodman, Steven N.},
  date = {1994-08-01},
  journaltitle = {Annals of Internal Medicine},
  volume = {121},
  pages = {200},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-121-3-199408010-00008},
  url = {http://annals.org/article.aspx?doi=10.7326/0003-4819-121-3-199408010-00008},
  urldate = {2020-07-24},
  file = {/Users/claudio/MEGA/Zotero/Goodman_1994_The Use of Predicted Confidence Intervals When Planning Experiments and the.pdf},
  langid = {english},
  number = {3}
}

@article{lenthStatisticalPowerCalculations12007,
  title = {Statistical Power Calculations1},
  author = {Lenth, R. V.},
  date = {2007-03-01},
  journaltitle = {Journal of Animal Science},
  shortjournal = {Journal of Animal Science},
  volume = {85},
  pages = {E24-E29},
  issn = {0021-8812},
  doi = {10.2527/jas.2006-449},
  url = {https://doi.org/10.2527/jas.2006-449},
  urldate = {2020-07-24},
  abstract = {This article focuses on how to do meaningful power calculations and sample-size determination for common study designs. There are 3 important guiding principles. First, certain types of retrospective power calculations should be avoided, because they add no new information to an analysis. Second, effect size should be specified on the actual scale of measurement, not on a standardized scale. Third, rarely can a definitive study be done without first doing a pilot study. Some simple examples as well as a complex example are given. Power calculations are illustrated using Java applets developed by the author.},
  issue = {suppl\_13}
}

@article{Senn1304,
  title = {Power Is Indeed Irrelevant in Interpreting Completed Studies},
  author = {Senn, Stephen J},
  date = {2002},
  journaltitle = {BMJ: British Medical Journal},
  shortjournal = {BMJ},
  volume = {325},
  pages = {1304},
  publisher = {{BMJ Publishing Group Ltd}},
  issn = {0959-8138},
  doi = {10.1136/bmj.325.7375.1304},
  url = {https://www.bmj.com/content/325/7375/1304.1},
  eprint = {https://www.bmj.com/content/325/7375/1304.1},
  number = {7375}
}


