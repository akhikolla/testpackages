<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Lampros Mouselimis" />

<meta name="date" content="2019-11-29" />

<title>Image classification of the MNIST and CIFAR-10 data using KernelKnn and HOG (histogram of oriented gradients)</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Image classification of the MNIST and CIFAR-10 data using KernelKnn and HOG (histogram of oriented gradients)</h1>
<h4 class="author">Lampros Mouselimis</h4>
<h4 class="date">2019-11-29</h4>



<p>In this vignette I’ll illustrate how to increase the accuracy on the MNIST (to approx. 98.4%) and CIFAR-10 data (to approx. 58.3%) using the KernelKnn package and HOG (histogram of oriented gradients). <br></p>
<div id="dependencies" class="section level3">
<h3>dependencies</h3>
<ul>
<li>The MNIST and Cifar-10 data sets can be downloaded from Github using <strong>system(“wget <a href="https://raw.githubusercontent.com/mlampros/DataSets/master/mnist.zip" class="uri">https://raw.githubusercontent.com/mlampros/DataSets/master/mnist.zip</a>”)</strong> and <strong>system(“wget <a href="https://raw.githubusercontent.com/mlampros/DataSets/master/cifar_10.zip" class="uri">https://raw.githubusercontent.com/mlampros/DataSets/master/cifar_10.zip</a>”)</strong></li>
<li>the <strong>irlba</strong> package, which is needed for comparison purposes, can be installed from CRAN directly</li>
<li>the <em>HOG_apply</em> function is part of the <strong>OpenImageR</strong> package, which can be installed from CRAN as well.</li>
</ul>
</div>
<div id="mnist-data-set" class="section level3">
<h3>MNIST data set</h3>
<p>The MNIST data set of handwritten digits has a training set of 70,000 examples and each row of the matrix corresponds to a 28 x 28 image. The unique values of the response variable <em>y</em> range from 0 to 9. More information about the data can be found in the <em>DataSets</em> repository (the folder includes also an Rmarkdown file).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># using system('wget..') on a linux OS </span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"></a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="kw">system</span>(<span class="st">&quot;wget https://raw.githubusercontent.com/mlampros/DataSets/master/mnist.zip&quot;</span>)             </a>
<a class="sourceLine" id="cb1-4" data-line-number="4"></a>
<a class="sourceLine" id="cb1-5" data-line-number="5">mnist &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="kw">unz</span>(<span class="st">&quot;mnist.zip&quot;</span>, <span class="st">&quot;mnist.csv&quot;</span>), <span class="dt">nrows =</span> <span class="dv">70000</span>, <span class="dt">header =</span> T, </a>
<a class="sourceLine" id="cb1-6" data-line-number="6">                    </a>
<a class="sourceLine" id="cb1-7" data-line-number="7">                    <span class="dt">quote =</span> <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">X =<span class="st"> </span>mnist[, <span class="op">-</span><span class="kw">ncol</span>(mnist)]</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb2-3" data-line-number="3"></a>
<a class="sourceLine" id="cb2-4" data-line-number="4"><span class="co">## [1] 70000   784</span></a>
<a class="sourceLine" id="cb2-5" data-line-number="5"></a>
<a class="sourceLine" id="cb2-6" data-line-number="6"><span class="co"># the KernelKnn function requires that the labels are numeric and start from 1 : Inf</span></a>
<a class="sourceLine" id="cb2-7" data-line-number="7"></a>
<a class="sourceLine" id="cb2-8" data-line-number="8">y =<span class="st"> </span>mnist[, <span class="kw">ncol</span>(mnist)] <span class="op">+</span><span class="st"> </span><span class="dv">1</span>          </a>
<a class="sourceLine" id="cb2-9" data-line-number="9"><span class="kw">table</span>(y)</a>
<a class="sourceLine" id="cb2-10" data-line-number="10"></a>
<a class="sourceLine" id="cb2-11" data-line-number="11"><span class="co">## y</span></a>
<a class="sourceLine" id="cb2-12" data-line-number="12"><span class="co">##    1    2    3    4    5    6    7    8    9   10 </span></a>
<a class="sourceLine" id="cb2-13" data-line-number="13"><span class="co">## 6903 7877 6990 7141 6824 6313 6876 7293 6825 6958</span></a></code></pre></div>
<p><br></p>
<p>K nearest neighbors do not perform well in high dimensions due to the <em>curse of dimensionality</em> (k observations that are nearest to a given test observation x1 may be very far away from x1 in p-dimensional space when p is large [ An introduction to statistical learning, James/Witten/Hastie/Tibshirani, pages 108-109 ]), leading to a very poor k-nearest-neighbors fit. One option to overcome this problem would be to use truncated svd (irlba package) to reduce the dimensions of the data. A second option, which is appropriate in case of images, would be to use image descriptors. In this vignette, I’ll compare those two approaches. <br><br></p>
<div id="kernelknncv-using-truncated-svd" class="section level5">
<h5>KernelKnnCV using truncated svd</h5>
<p>I experimented with different settings and the following parameters gave the best results, <br><br></p>
<table>
<thead>
<tr class="header">
<th align="left">irlba_singlular_vectors</th>
<th align="left">k</th>
<th align="left">method</th>
<th align="left">kernel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">40</td>
<td align="left">8</td>
<td align="left">braycurtis</td>
<td align="left">biweight_tricube_MULT</td>
</tr>
<tr class="even">
<td align="left"><br></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">library</span>(irlba)</a>
<a class="sourceLine" id="cb3-2" data-line-number="2"></a>
<a class="sourceLine" id="cb3-3" data-line-number="3">svd_irlb =<span class="st"> </span><span class="kw">irlba</span>(<span class="kw">as.matrix</span>(X), <span class="dt">nv =</span> <span class="dv">40</span>, <span class="dt">nu =</span> <span class="dv">40</span>, <span class="dt">verbose =</span> F)            <span class="co"># irlba truncated svd</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4"></a>
<a class="sourceLine" id="cb3-5" data-line-number="5">new_x =<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>svd_irlb<span class="op">$</span>v               <span class="co"># new_x using the 40 right singular vectors</span></a></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">library</span>(KernelKnn)</a>
<a class="sourceLine" id="cb4-2" data-line-number="2"></a>
<a class="sourceLine" id="cb4-3" data-line-number="3">fit =<span class="st"> </span><span class="kw">KernelKnnCV</span>(<span class="kw">as.matrix</span>(new_x), y, <span class="dt">k =</span> <span class="dv">8</span>, <span class="dt">folds =</span> <span class="dv">4</span>, <span class="dt">method =</span> <span class="st">'braycurtis'</span>,</a>
<a class="sourceLine" id="cb4-4" data-line-number="4">                  </a>
<a class="sourceLine" id="cb4-5" data-line-number="5">                  <span class="dt">weights_function =</span> <span class="st">'biweight_tricube_MULT'</span>, <span class="dt">regression =</span> F, </a>
<a class="sourceLine" id="cb4-6" data-line-number="6">                  </a>
<a class="sourceLine" id="cb4-7" data-line-number="7">                  <span class="dt">threads =</span> <span class="dv">6</span>, <span class="dt">Levels =</span> <span class="kw">sort</span>(<span class="kw">unique</span>(y)))</a>
<a class="sourceLine" id="cb4-8" data-line-number="8"></a>
<a class="sourceLine" id="cb4-9" data-line-number="9"></a>
<a class="sourceLine" id="cb4-10" data-line-number="10"><span class="co"># str(fit)</span></a>
<a class="sourceLine" id="cb4-11" data-line-number="11"></a>
<a class="sourceLine" id="cb4-12" data-line-number="12"></a>
<a class="sourceLine" id="cb4-13" data-line-number="13"><span class="co"># evaluation metric</span></a>
<a class="sourceLine" id="cb4-14" data-line-number="14"></a>
<a class="sourceLine" id="cb4-15" data-line-number="15">acc =<span class="st"> </span><span class="cf">function</span> (y_true, preds) {</a>
<a class="sourceLine" id="cb4-16" data-line-number="16">  </a>
<a class="sourceLine" id="cb4-17" data-line-number="17">  out =<span class="st"> </span><span class="kw">table</span>(y_true, <span class="kw">max.col</span>(preds, <span class="dt">ties.method =</span> <span class="st">&quot;random&quot;</span>))</a>
<a class="sourceLine" id="cb4-18" data-line-number="18">  </a>
<a class="sourceLine" id="cb4-19" data-line-number="19">  acc =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">diag</span>(out))<span class="op">/</span><span class="kw">sum</span>(out)</a>
<a class="sourceLine" id="cb4-20" data-line-number="20">  </a>
<a class="sourceLine" id="cb4-21" data-line-number="21">  acc</a>
<a class="sourceLine" id="cb4-22" data-line-number="22">}</a></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">acc_fit =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit<span class="op">$</span>preds), </a>
<a class="sourceLine" id="cb5-2" data-line-number="2">                        </a>
<a class="sourceLine" id="cb5-3" data-line-number="3">                        <span class="cf">function</span>(x) <span class="kw">acc</span>(y[fit<span class="op">$</span>folds[[x]]], </a>
<a class="sourceLine" id="cb5-4" data-line-number="4">                                        </a>
<a class="sourceLine" id="cb5-5" data-line-number="5">                                        fit<span class="op">$</span>preds[[x]])))</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">acc_fit</a>
<a class="sourceLine" id="cb5-7" data-line-number="7"></a>
<a class="sourceLine" id="cb5-8" data-line-number="8"><span class="co">## [1] 0.9742857 0.9749143 0.9761143 0.9741143</span></a>
<a class="sourceLine" id="cb5-9" data-line-number="9"></a>
<a class="sourceLine" id="cb5-10" data-line-number="10"><span class="kw">cat</span>(<span class="st">'mean accuracy using cross-validation :'</span>, <span class="kw">mean</span>(acc_fit), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb5-11" data-line-number="11"></a>
<a class="sourceLine" id="cb5-12" data-line-number="12"><span class="co">## mean accuracy using cross-validation : 0.9748571</span></a></code></pre></div>
<p><br></p>
<p>Utilizing truncated svd a 4-fold cross-validation KernelKnn model gives a 97.48% accuracy. <br><br></p>
</div>
<div id="kernelknncv-and-hog-histogram-of-oriented-gradients" class="section level5">
<h5>KernelKnnCV and HOG (histogram of oriented gradients)</h5>
<p>In this chunk of code, besides KernelKnnCV I’ll also use HOG. The histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The technique counts occurrences of gradient orientation in localized portions of an image. This method is similar to that of edge orientation histograms, scale-invariant feature transform descriptors, and shape contexts, but differs in that it is computed on a dense grid of uniformly spaced cells and uses overlapping local contrast normalization for improved accuracy (Wikipedia). <br></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">library</span>(OpenImageR)</a>
<a class="sourceLine" id="cb6-2" data-line-number="2"></a>
<a class="sourceLine" id="cb6-3" data-line-number="3">hog =<span class="st"> </span><span class="kw">HOG_apply</span>(X, <span class="dt">cells =</span> <span class="dv">6</span>, <span class="dt">orientations =</span> <span class="dv">9</span>, <span class="dt">rows =</span> <span class="dv">28</span>, <span class="dt">columns =</span> <span class="dv">28</span>, <span class="dt">threads =</span> <span class="dv">6</span>)</a>
<a class="sourceLine" id="cb6-4" data-line-number="4"></a>
<a class="sourceLine" id="cb6-5" data-line-number="5"><span class="co">## </span></a>
<a class="sourceLine" id="cb6-6" data-line-number="6"><span class="co">## time to complete : 1.802997 secs</span></a>
<a class="sourceLine" id="cb6-7" data-line-number="7"></a>
<a class="sourceLine" id="cb6-8" data-line-number="8"><span class="kw">dim</span>(hog)</a>
<a class="sourceLine" id="cb6-9" data-line-number="9"></a>
<a class="sourceLine" id="cb6-10" data-line-number="10"><span class="co">## [1] 70000   324</span></a></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">fit_hog =<span class="st"> </span><span class="kw">KernelKnnCV</span>(hog, y, <span class="dt">k =</span> <span class="dv">20</span>, <span class="dt">folds =</span> <span class="dv">4</span>, <span class="dt">method =</span> <span class="st">'braycurtis'</span>,</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">                  </a>
<a class="sourceLine" id="cb7-3" data-line-number="3">                  <span class="dt">weights_function =</span> <span class="st">'biweight_tricube_MULT'</span>, <span class="dt">regression =</span> F, </a>
<a class="sourceLine" id="cb7-4" data-line-number="4">                  </a>
<a class="sourceLine" id="cb7-5" data-line-number="5">                  <span class="dt">threads =</span> <span class="dv">6</span>, <span class="dt">Levels =</span> <span class="kw">sort</span>(<span class="kw">unique</span>(y)))</a>
<a class="sourceLine" id="cb7-6" data-line-number="6"></a>
<a class="sourceLine" id="cb7-7" data-line-number="7"></a>
<a class="sourceLine" id="cb7-8" data-line-number="8"><span class="co">#str(fit_hog)</span></a></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1">acc_fit_hog =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit_hog<span class="op">$</span>preds), </a>
<a class="sourceLine" id="cb8-2" data-line-number="2">                            </a>
<a class="sourceLine" id="cb8-3" data-line-number="3">                            <span class="cf">function</span>(x) <span class="kw">acc</span>(y[fit_hog<span class="op">$</span>folds[[x]]], </a>
<a class="sourceLine" id="cb8-4" data-line-number="4">                                            </a>
<a class="sourceLine" id="cb8-5" data-line-number="5">                                            fit_hog<span class="op">$</span>preds[[x]])))</a>
<a class="sourceLine" id="cb8-6" data-line-number="6">acc_fit_hog</a>
<a class="sourceLine" id="cb8-7" data-line-number="7"></a>
<a class="sourceLine" id="cb8-8" data-line-number="8"><span class="co">## [1] 0.9833714 0.9840571 0.9846857 0.9838857</span></a>
<a class="sourceLine" id="cb8-9" data-line-number="9"></a>
<a class="sourceLine" id="cb8-10" data-line-number="10"><span class="kw">cat</span>(<span class="st">'mean accuracy for hog-features using cross-validation :'</span>, <span class="kw">mean</span>(acc_fit_hog), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb8-11" data-line-number="11"></a>
<a class="sourceLine" id="cb8-12" data-line-number="12"><span class="co">## mean accuracy for hog-features using cross-validation : 0.984</span></a></code></pre></div>
<p><br></p>
<p>By changing from the simple svd-features to HOG-features the accuracy of a 4-fold cross-validation model increased from 97.48% to 98.4% (approx. 1% difference)</p>
</div>
</div>
<div id="cifar-10-data-set" class="section level3">
<h3>CIFAR-10 data set</h3>
<p>CIFAR-10 is an established computer-vision dataset used for object recognition. The data I’ll use in this example is a subset of an 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes ( 6000 images per class ). Furthermore, the data were converted from RGB to gray, normalized and rounded to 2 decimal places (to reduce the storage size). More information about the data can be found in my <em>DataSets</em> repository (I included an Rmarkdown file). <br><br></p>
<p>I’ll build the kernel k-nearest-neighbors models in the same way I’ve done for the mnist data set and then I’ll compare the results.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="co"># using system('wget..') on a linux OS </span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2"></a>
<a class="sourceLine" id="cb9-3" data-line-number="3"><span class="kw">system</span>(<span class="st">&quot;wget https://raw.githubusercontent.com/mlampros/DataSets/master/cifar_10.zip&quot;</span>)      </a>
<a class="sourceLine" id="cb9-4" data-line-number="4"></a>
<a class="sourceLine" id="cb9-5" data-line-number="5">cifar_<span class="dv">10</span> &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="kw">unz</span>(<span class="st">&quot;cifar_10.zip&quot;</span>, <span class="st">&quot;cifar_10.csv&quot;</span>), <span class="dt">nrows =</span> <span class="dv">60000</span>, <span class="dt">header =</span> T, </a>
<a class="sourceLine" id="cb9-6" data-line-number="6">                       </a>
<a class="sourceLine" id="cb9-7" data-line-number="7">                       <span class="dt">quote =</span> <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>)</a></code></pre></div>
<div id="kernelknncv-using-truncated-svd-1" class="section level5">
<h5>KernelKnnCV using truncated svd</h5>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1">X =<span class="st"> </span>cifar_<span class="dv">10</span>[, <span class="op">-</span><span class="kw">ncol</span>(cifar_<span class="dv">10</span>)]</a>
<a class="sourceLine" id="cb10-2" data-line-number="2"><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb10-3" data-line-number="3"></a>
<a class="sourceLine" id="cb10-4" data-line-number="4"><span class="co">## [1] 60000  1024</span></a>
<a class="sourceLine" id="cb10-5" data-line-number="5"></a>
<a class="sourceLine" id="cb10-6" data-line-number="6"><span class="co"># the KernelKnn function requires that the labels are numeric and start from 1 : Inf</span></a>
<a class="sourceLine" id="cb10-7" data-line-number="7"></a>
<a class="sourceLine" id="cb10-8" data-line-number="8">y =<span class="st"> </span>cifar_<span class="dv">10</span>[, <span class="kw">ncol</span>(cifar_<span class="dv">10</span>)]         </a>
<a class="sourceLine" id="cb10-9" data-line-number="9"><span class="kw">table</span>(y)</a>
<a class="sourceLine" id="cb10-10" data-line-number="10"></a>
<a class="sourceLine" id="cb10-11" data-line-number="11"><span class="co">## y</span></a>
<a class="sourceLine" id="cb10-12" data-line-number="12"><span class="co">##    1    2    3    4    5    6    7    8    9   10 </span></a>
<a class="sourceLine" id="cb10-13" data-line-number="13"><span class="co">## 6000 6000 6000 6000 6000 6000 6000 6000 6000 6000</span></a></code></pre></div>
<p><br></p>
<p>The parameter settings are similar to those for the mnist data,</p>
<table>
<thead>
<tr class="header">
<th align="left">irlba_singlular_vectors</th>
<th align="left">k</th>
<th align="left">method</th>
<th align="left">kernel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">40</td>
<td align="left">8</td>
<td align="left">braycurtis</td>
<td align="left">biweight_tricube_MULT</td>
</tr>
<tr class="even">
<td align="left"><br></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1">svd_irlb =<span class="st"> </span><span class="kw">irlba</span>(<span class="kw">as.matrix</span>(X), <span class="dt">nv =</span> <span class="dv">40</span>, <span class="dt">nu =</span> <span class="dv">40</span>, <span class="dt">verbose =</span> F)            <span class="co"># irlba truncated svd</span></a>
<a class="sourceLine" id="cb11-2" data-line-number="2"></a>
<a class="sourceLine" id="cb11-3" data-line-number="3">new_x =<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>svd_irlb<span class="op">$</span>v               <span class="co"># new_x using the 40 right singular vectors</span></a></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1">fit =<span class="st"> </span><span class="kw">KernelKnnCV</span>(<span class="kw">as.matrix</span>(new_x), y, <span class="dt">k =</span> <span class="dv">8</span>, <span class="dt">folds =</span> <span class="dv">4</span>, <span class="dt">method =</span> <span class="st">'braycurtis'</span>,</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">                  </a>
<a class="sourceLine" id="cb12-3" data-line-number="3">                  <span class="dt">weights_function =</span> <span class="st">'biweight_tricube_MULT'</span>, <span class="dt">regression =</span> F,</a>
<a class="sourceLine" id="cb12-4" data-line-number="4">                  </a>
<a class="sourceLine" id="cb12-5" data-line-number="5">                  <span class="dt">threads =</span> <span class="dv">6</span>, <span class="dt">Levels =</span> <span class="kw">sort</span>(<span class="kw">unique</span>(y)))</a>
<a class="sourceLine" id="cb12-6" data-line-number="6"></a>
<a class="sourceLine" id="cb12-7" data-line-number="7"></a>
<a class="sourceLine" id="cb12-8" data-line-number="8"><span class="co"># str(fit)</span></a></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1">acc_fit =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit<span class="op">$</span>preds),</a>
<a class="sourceLine" id="cb13-2" data-line-number="2">                        </a>
<a class="sourceLine" id="cb13-3" data-line-number="3">                        <span class="cf">function</span>(x) <span class="kw">acc</span>(y[fit<span class="op">$</span>folds[[x]]], </a>
<a class="sourceLine" id="cb13-4" data-line-number="4">                                        </a>
<a class="sourceLine" id="cb13-5" data-line-number="5">                                        fit<span class="op">$</span>preds[[x]])))</a>
<a class="sourceLine" id="cb13-6" data-line-number="6">acc_fit</a>
<a class="sourceLine" id="cb13-7" data-line-number="7"></a>
<a class="sourceLine" id="cb13-8" data-line-number="8"><span class="co">## [1] 0.4080667 0.4097333 0.4040000 0.4102667</span></a>
<a class="sourceLine" id="cb13-9" data-line-number="9"></a>
<a class="sourceLine" id="cb13-10" data-line-number="10"><span class="kw">cat</span>(<span class="st">'mean accuracy using cross-validation :'</span>, <span class="kw">mean</span>(acc_fit), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb13-11" data-line-number="11"></a>
<a class="sourceLine" id="cb13-12" data-line-number="12"><span class="co">## mean accuracy using cross-validation : 0.4080167</span></a></code></pre></div>
<p><br></p>
<p>The accuracy of a 4-fold cross-validation model using truncated svd is 40.8%.</p>
</div>
<div id="kernelknncv-using-hog-histogram-of-oriented-gradients" class="section level5">
<h5>KernelKnnCV using HOG (histogram of oriented gradients)</h5>
<p><br> Next, I’ll run the KernelKnnCV using the HOG-descriptors, <br><br></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">hog =<span class="st"> </span><span class="kw">HOG_apply</span>(X, <span class="dt">cells =</span> <span class="dv">6</span>, <span class="dt">orientations =</span> <span class="dv">9</span>, <span class="dt">rows =</span> <span class="dv">32</span>,</a>
<a class="sourceLine" id="cb14-2" data-line-number="2">                </a>
<a class="sourceLine" id="cb14-3" data-line-number="3">                <span class="dt">columns =</span> <span class="dv">32</span>, <span class="dt">threads =</span> <span class="dv">6</span>)</a>
<a class="sourceLine" id="cb14-4" data-line-number="4"></a>
<a class="sourceLine" id="cb14-5" data-line-number="5"><span class="co">## </span></a>
<a class="sourceLine" id="cb14-6" data-line-number="6"><span class="co">## time to complete : 3.394621 secs</span></a>
<a class="sourceLine" id="cb14-7" data-line-number="7"></a>
<a class="sourceLine" id="cb14-8" data-line-number="8"><span class="kw">dim</span>(hog)</a>
<a class="sourceLine" id="cb14-9" data-line-number="9"></a>
<a class="sourceLine" id="cb14-10" data-line-number="10"><span class="co">## [1] 60000   324</span></a></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1">fit_hog =<span class="st"> </span><span class="kw">KernelKnnCV</span>(hog, y, <span class="dt">k =</span> <span class="dv">20</span>, <span class="dt">folds =</span> <span class="dv">4</span>, <span class="dt">method =</span> <span class="st">'braycurtis'</span>,</a>
<a class="sourceLine" id="cb15-2" data-line-number="2">                  </a>
<a class="sourceLine" id="cb15-3" data-line-number="3">                  <span class="dt">weights_function =</span> <span class="st">'biweight_tricube_MULT'</span>, <span class="dt">regression =</span> F,</a>
<a class="sourceLine" id="cb15-4" data-line-number="4">                  </a>
<a class="sourceLine" id="cb15-5" data-line-number="5">                  <span class="dt">threads =</span> <span class="dv">6</span>, <span class="dt">Levels =</span> <span class="kw">sort</span>(<span class="kw">unique</span>(y)))</a>
<a class="sourceLine" id="cb15-6" data-line-number="6"></a>
<a class="sourceLine" id="cb15-7" data-line-number="7"></a>
<a class="sourceLine" id="cb15-8" data-line-number="8"><span class="co"># str(fit_hog)</span></a></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1">acc_fit_hog =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit_hog<span class="op">$</span>preds), </a>
<a class="sourceLine" id="cb16-2" data-line-number="2">                            </a>
<a class="sourceLine" id="cb16-3" data-line-number="3">                            <span class="cf">function</span>(x) <span class="kw">acc</span>(y[fit_hog<span class="op">$</span>folds[[x]]], </a>
<a class="sourceLine" id="cb16-4" data-line-number="4">                                            </a>
<a class="sourceLine" id="cb16-5" data-line-number="5">                                            fit_hog<span class="op">$</span>preds[[x]])))</a>
<a class="sourceLine" id="cb16-6" data-line-number="6">acc_fit_hog</a>
<a class="sourceLine" id="cb16-7" data-line-number="7"></a>
<a class="sourceLine" id="cb16-8" data-line-number="8"><span class="co">## [1] 0.5807333 0.5884000 0.5777333 0.5861333</span></a>
<a class="sourceLine" id="cb16-9" data-line-number="9"></a>
<a class="sourceLine" id="cb16-10" data-line-number="10"><span class="kw">cat</span>(<span class="st">'mean accuracy for hog-features using cross-validation :'</span>, <span class="kw">mean</span>(acc_fit_hog), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb16-11" data-line-number="11"></a>
<a class="sourceLine" id="cb16-12" data-line-number="12"><span class="co">## mean accuracy for hog-features using cross-validation : 0.58325</span></a></code></pre></div>
<p><br></p>
<p>By using hog-descriptors in a 4-fold cross-validation model the accuracy in the cifar-10 data increases from 40.8% to 58.3% (approx. 17.5% difference).</p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
